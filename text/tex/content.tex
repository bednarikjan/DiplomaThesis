%=========================================================================

%- potreba mit 10-20 vysazenych stran / 20-40 normostran
%- na jednu plne potistenou stranu se vleze 1,65 (s nadpisem chapter) nebo 2,36 (bez nadpisu) normostran

% Setting the depth of sections numbering
\setcounter{secnumdepth}{2}
% Setting the depth of sections to appear in the table of contents.
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}


%=========================================================================
%=========================================================================
\chapter{Introduction} \label{txt:introduction}

\todo{Abstract by mel mit max 10 radku.}
\todo{zasazení řešené problematiky do širšího kontextu}
\todo{ávaznost diplomové práce na Semestrální projekt a vymezí objem prací, které byly do diplomové práce převzaty, včetně odkazů do kapitol diplomové práce ???}
\todo{aplikace systemu, vyhody, nevyhody}
\todo{historie}
\todo{overview co system dela}
\todo{uvest,z e cil muze byt pomerne maly v obraze (par pixelu)}
\todo{pridat par paragraphs o hlavnich komponentach systemu}
\todo{pridat tabulku requirements pro tracking}

In todays world the localization of the aerial objects is an essential component of two main domains, a public air traffic control (ATC) and a national defense. In both cases the main purpose of the system is to estimate the distance and the 3D position of a given airborne vehicle.

In case of the ATC, the airports mainly rely on the multilateration systems which specialize on surveillance and control of air traffic during all flight phases \cite{Gaviria:newStrategiesMLAT}. The design of such a system expects that the aircraft are equipped with a secondary surveillance radar transponders which periodically emit the signals to the ground receiving stations, however primary radars are widely used as well \cite{Airtrafficmuseum}. In the military segment the aerial object localization mainly serves the purpose of detecting and identifying a intruder which might pose a threat for a given area under protection. In this scenario only primary radars are used since the aerial objects are not expected to cooperate.

Even though being widely used, radar based localization systems suffer from several drawbacks. Radar is a device based on the active emission of the radio signal \cite{toomay2012radar}. In order to localize distant objects, a huge amount of energy must be radiated to make sure it would return from the target and a small amount of energy returned might be easily disrupted \cite{Airtrafficmuseum}. What is more, in defense applications it is not desirable that the tracked object would find out that it is being tracked, which is the condition an actively radiating system, such as a radar, cannot achieve. Last but not least, the professional class radars in use by both public segment and military are in general expensive, large and heavy devices not suitable for mobile deployment.

In this work, I propose a fully autonomous passive multi-camera system for detecting, tracking and localizing aerial objects which is based merely on ordinary RGB cameras --- the Optical Localization System (OLS). Since the system does not rely on active emission of a signal, but rather captures the optical information from the environment, it can be used for secret localization of aerial targets. The system is designed to be well suited for mobility and temporary deployment since each camera station used weigh no more than twenty kilograms and the whole system is relatively inexpensive as well.

\paragraph{object tracking}
Requirements: long-term tracking (partial occlusion, temporal disappearance from scene, ...), multi-hypothesis - possible simultaneous tracking of multiple targets, suitable representation of very small targets (which mich change over time), real-time (at least 10 FPS), moving cameras.

\begin{figure}[t]\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\centering
	\includegraphics[width=0.45\linewidth, height=5cm]{fig/small_tracked_object.png}
	\caption{\todo{Dodat caption}}
	\label{fig:small_tracked_object}
\end{figure}


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%\section{Background} \label{txt:background}

%%% Historie

The principle of the optical localization of objects, which is based on triangulation, is well known for quite a long time. The first devices which were designed to allow an operator to estimate a distance to a given object using mechanical and optical principles --- optical range finders --- emerged in the second half of the 18th century \cite{bud1998instruments}. Ever since, the optical range finders had been mostly used for military operations in order to estimate the position of either naval, airborne or terrestrial targets until the World War II when radar was invented.

The OLS system aims to utilize the same principle as the old optical range finders. However, instead of an optical device requiring the operator to aim on the target manually the OLS takes advantage of the RGB cameras and the image processing techniques capable of detecting, tracking and localizing the target autonomously without the need for a human to interfere. Furthermore, with the use of sensors capable of finding a geographical position of each observer unit the OLS system estimates a global geographical position of a given target (not only the distance or relative position).

%%% Princip lokalizace

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%\section{Principle of Object Localisation}
\todo{Obecne jak to funguje}

\begin{figure*}[t]\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\centering
	\includegraphics[width=0.45\linewidth, height=5cm]{fig/placeholder.pdf}
	\includegraphics[width=0.45\linewidth, height=5cm]{fig/placeholder.pdf}
	\caption{\todo{Dodat caption}}
	\label{fig:WidePicture}
\end{figure*}

\vata[4]

%=========================================================================
%=========================================================================
\chapter{Related Work}

Being a complex and semi-autonomously working system, the multi-camera optical localization system comprises problematics ranging over several different areas. Most importantly, robust visual tracking capable of long-term tracking of arbitrary target which might exhibit time varying appearance and which might move in a cluttered environment must be employed. Furthermore, a suitable approach for estimating the position of the target in 3-space given noisy measurements must be proposed. Finally a possible extension of occlusion prediction using 3D environment reconstruction. The overview of visual tracking approaches and the discussion of their suitability for OLS is given in Section~\ref{txt:object_tracking}, the most widely used methods for localization in 3-space given projective geometry are described in Section~\ref{txt:multi_camera_target_localization} and the problematics of 3D envrironment reconstruction is introduced in Section~\ref{txt:3d_environment_reconstruction}.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%\section{Moving Object Detection}
%
%Before a tracking of the object can be initiated, the object of interest must first be discovered, which is the main task of object detectors. Even though the OLS allows a human operator to interfere and manually select an object for tracking, the system should be able to perform fully autonomously as well. The approaches to detect an object can be divided into two categories based on the appearance of the object, where the first group covers the cases where the system already posses a strong information about the objects since they incorporate artificial landmarks, while the second group relies merely on the natural appearance and has only limited or no prior knowledge about the objects \cite{Multi-Camera_Sensor_System_for_3D_Segmentation}. Since the OLS aims on tracking unknown UAVs the first group of approaches is out of question.
%
%A couple of approaches can be distinguished among the detectors using natural appearance \cite{Yilmaz:2006:OTS:1177352.1177355}:
%
%\paragraph{detection of points} This approach goes with the shape representation of the object given by the keypoints. Among others the Harris corner detector or SIFT and SURF descriptors are widely used.
%
%\paragraph{background modeling} Under the assumption an observed scene does not change rapidly its appearance can be learned resulting in a background model. The task of the object detector is then to estimate for each subregion or even each pixel whether it belongs to a background or to a foreground (i.e. the object of interest). A widely used approach is to model each pixel as a mixture of Gaussians (in order to support a periodically changing background). Another possible solution is to model intensity variations of separate pixels as the states (including both \textit{background} and \textit{foreground} state) and then with the help of Hidden Markov Models to estimate the state a given pixel is currently in. Since it is possible to enhance these algorithms to allow for the camera motion (resulting in the change of the camera view) they seem suitable for the OLS system. 
%
%\paragraph{supervised classification} If the (coarse) class of the objects to be tracked is known beforehand and a proper dataset is available, a classifier can be trained to detect the objects. Among others, the neural networks or adaptive boosting methods are widely used. Since the OLS aims to localize UAVs, the utilization of a classifier seems like a suitable solution.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Object Tracking} \label{txt:object_tracking}

This section discusses the various approaches to tracking of the objects using the computer vision techniques. First the importance of the suitable object representation is explained and the properties of various object models as well as their advantages and disadvantages with regards to the requirements of the OLS system are discussed. Different categories of tracking algorithms are then described and two specific approaches which are most appropriate for OLS -- the \textit{TLD tracker} and the \textit{foreground-background tracker based on the particle filter framework} -- are explained in more detail.

%.........................................................................
%.........................................................................
\subsection{Object Model} \label{txt:object_model}

The choice of how the targets are represented determines the domain of approaches used for visual detection and/or tracking due to the strong relationship between the algorithm and the object model. Neither of the stat-of-the-art approaches is universal enough to cope with all the difficulties and disturbances, such as the illumination change, occlusion, cluttered background, motion blurring, appearance change due to deformation and/or transformation of the object \cite{Li:2013:SAM:2508037.2508039}, which might occur over the course of tracking. Therefore, the model should suit a priori known tracking conditions (e.g. size, speed, rigidity and motion model of a target, number of targets, camera motion, background model etc.). Yilmaz et al. classified the model representations into two categories in their review \cite{Yilmaz:2006:OTS:1177352.1177355} -- the \textit{shape} and the \textit{appearance}. 

%Beyond these two classes, a few categories of widely used image features can be distinguished, e.g. \textit{gradient, texture or spatio-temporal features}, with some of them being specifically developed for tracking certain classes of objects, such as pedestrians or cars \cite{Yang20113823}.

\paragraph{shape representation} 
A \textit{shape} model encompasses points \cite{Tomasi91detectionand}, contours \cite{Kass88snakes:active, ActiveContour-BasedVisualTracking} or articulated models \cite{Delamarre2001328, conf/isvc/MigniotA13} (see Figure \ref{fig:object_models}). The point representation is not suitable for OLS since the distant aerial objects appear relatively small in the image and might not provide enough distinctive points (see Figure \ref{fig:small_tracked_object}. Both contours and articulated models are mostly used for tracking non-rigid deformable objects which is not the main concern of OLS. Additinally the accuracy of fitting a contour to a target strongly depends on the convergence criteria of the energy minimization function, thus they might be computationally expensive.

\paragraph{appearance representation} 
An \textit{appearance} model is represented either by rectangular template \cite{ProbabilisticVisualTrackingTemplate, ObjectTrackinginMonochromaticVideo} or a weighted kernel \cite{Comaniciu:2003:KOT:776753.776799, MultipleCollaborativeKernelTracking} (see Figure \ref{fig:object_models}). The main advantage of both representations is the fact they contain both the spatial and appearance information, additionally they scale well to varying object size (approaching and receding object). The appearance model seem to suit the requirements of OLS, thus the tracking approaches based on the variants of both template and kernel model will be used (see Sections \ref{txt:tracking_learning_detection} and \ref{txt:bgfg_tracker}).

%% Object models - shape and appearance representation.
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.7\textwidth]{fig/object_model.png}
	\caption{Various approaches to tracked object modeling. (a) Keypoints, (b) contour and (c) articulated model fall into the \textit{shape representation} category whereas (d) template and (e) kernel belong to the \textit{appearance representation} category. The intensity of a red color in case (e) denotes the weight of the given pixel in the given (ellipse shaped) kernel.}
	\label{fig:object_models}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Tracking approaches}

The main purpose of the tracker is to iteratively estimate the trajectory of the tracked object from frame to frame. There is a wide range of approaches to visual tracking and since they usually combine multiple various methods in order to reinforce the tracker robustness they cannot be really divided into distinct categories in a straightforward manner. However, the approaches can be coarsely categorized by the selection of the object representation.

To reinforce the tracker robustness, the motion models are often used, Kalman filter and particle filter being the most popular ones \cite{cuevas2005kalman, ObjectTrackinginMonochromaticVideo}.


\subsubsection*{Keypoint Tracking} 
Keypoint tracking represents one of the most common approaches \cite{Tomasi91detectionand,Nebehay2014WACV}. The \textit{keypoint} is understood to be a single point in a an image, which represents a small image region -- a \textit{point descriptor} -- which should be highly discriminative and invariant to various image transformations. There are many widely used keypoint detectors/descriptors e.g. SIFT, SURF, ORB, FREAK, etc. \cite{sift, Bay:2008:SRF:1370312.1370556, Rublee:2011:OEA:2355573.2356268, Ortiz:2012:FFR:2354409.2354903} and they differ mainly in the means of matching precision and computation speed \cite{Schaeffer_acomparison, conf/icpr/MiksikM12}. What the tracker does is that it detects the keypoints and their descriptors in each frame, selects those representing the object, finds the correspondences and computes the transformation from the previous frame. Even though the keypoint tracking is well established approach, it cannot be used in OLS due to the insufficient size of the tracked objects, as was explained in Section \ref{txt:object_model}.

\subsubsection*{Kernel Tracking}
Kernel approaches are based on a so-called \textit{kernel}. Basically, the feature target representation is spatially masked with an isotropic kernel (for illustration see Figure \ref{fig:object_models} (e)), which assigns the largest weights to the pixels in the middle while the weights decay in the directions towards the edges of the kernel. This enables a spatially-smooth similarity function to be defined. Consequently, this function can be optimized in the means of target position using traditional gradient based methods such as gradient descent \cite{Comaniciu:2003:KOT:776753.776799}. To boost the robustness of the tracker multiple collaborative kernels might be used \cite{MultipleCollaborativeKernelTracking, Multi-kernelCorrelationFilterForVisualTracking}. The notion of distinguishing which pixels in the kernel/template are more or less reliable is also utilized in one of the approaches the OLS is based on -- the background/foreground tracker (see Section \ref{txt:bgfg_tracker}).

\subsubsection*{Tracking-by-Detection}
This class of approaches heavily utilize the detection principles in combination with motion based approaches to localize the object \cite{eth_biwi_00633, Kalal:2012:TRA:2225045.2225082}. Depending on the object model, the detection might be performed either by detecting keypoints and matching them against the pretrained model \cite{FastKeypointRecognitionInTenLinesOfCode, DBLP:journals/corr/abs-1211-5829}, or by dividing the image into individual patches in which the object is searched for. For each patch, the template matching is performed \cite{AFastTemplateMatchingAlgorithm, ObjectTrackinginMonochromaticVideo} (using SSD\footnote{Sum of Squared Differences.}, SAD\footnote{Sum of Absolute Differences.} or NCC\footnote{Normalized Cross Correlation.} as a similarity measure) or feature set is extracted; consequently, the model presence probability is evaluated using the generative or the discriminative classifier \cite{EnhancedGaussianMixtureModels, EfficientScan-windowGPGPU}. Since the exhaustive search within the whole image is computationally expensive, the cascade classifiers might be applied \cite{violaJones, RobustObjectDetectionViaSoftCascade}. The TLD approach, which is used in OLS, utilizes the object detector in order to correct and/or reinitialize the tracker (see Section \ref{txt:tracking_learning_detection}).

\subsubsection*{Motion Based Tracking}
This category of approaches attempts to extract the motion occurring between the consecutive images. The \textit{optical flow} method which in general attempts to find the motion of individual pixels in the image can be used to track the keypoints \cite{Bouguet00pyramidalimplementation} or to produce the binary feature images and consequently the blobs corresponding to moving objects \cite{aslani2013optical}. Alternatively, the moving object can be detected in the image regions yielding the highest response of \textit{frame differencing} (known also as \textit{background subtraction}) \cite{Noh2013, Movingobjectdetection}. The background/foreground tracker which OLS uses is based on the frame differencing to estimate the appearance model and most likely location of the tracked object (see Section \ref{txt:bgfg_tracker}).

\subsubsection*{Motion Modeling}
The tracking can be approached through the model of a discrete-time dynamic system, where the aim is to estimate the current state for each incoming frame \cite{Comaniciu:2003:KOT:776753.776799}. The state can be represented as a mere 2D position of a target (in pixel-coordinates) or other parameters such as a velocity or an acceleration of a target can be modeled a well. Thanks to the motion model the (computationally expensive) exhaustive search for the object can be reduced to the vicinity of the current target position estimate.

\paragraph{Kalman filter} 
Kalman filtering is one of the widely used technique for recursively evaluating the current state of a target given the measurement corrupted by the measurement noise and the prediction of the next state corrupted by the process noise \cite{Welch:1995:IKF:897831, cuevas2005kalman}. It is based on the assumption that the state posterior density is Gaussian and thus can be parametrized by means and covariances. However, this assumption might not hold. In case of OLS a camera used for tracking is often in motion and the sensory data about its position which could be used to stabilize the motion might be imprecise or not available at all (for illustration see Figure \ref{fig:kalman}). Consequently, the position of a tracked object can change rapidly from frame to frame and thus to defy the assumption of the Gaussian distribution of the state posterior density. Furthermore the basic Kalman filter is based on a unimodal Gaussian distribution which prevents it from keeping multiple hypothesis for a single target.

%% Kalman filter illustration
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.95\textwidth]{fig/kalman.pdf}
	\caption{The evolution of the measurement and prediction probability density of the tracked target in the Kalman filter framework. The target apparently moves along a line with constant velocity, however, in frame $k = 4$ it suddenly changes its position due to the rapid camera movement which si something that Kalman filter cannot cope with.}
	\label{fig:kalman}
\end{figure}

\paragraph{particle filter} 
The particle filter represents the most general class of filters which can cope with non-Gaussian state and measurement processes as well as with tracking multiple hypothesis \cite{journals/cviu/BimboD11}. Current state of the system is represented as a \textit{particle} $\particle{t}{i}$ -- a vector of parameters ${\vec{x_{t}^{i}}}$ describing the properties of tracked object (e.g. position, velocity, acceleration, etc.) and assigned scalar weight $w_{t}^{i}$. The suitable \textit{fitness function} must be proposed which evaluates how well a particle fits to the observed data. Using finite number of particles the particle filter basically samples the fitness function (which might be arbitrarily complex and non-differentiable) in an attempt to find the optimum. A bootstrap particle filter (BPF) is a variant of particle filter widely used for visual tracking \cite{Isard98condensation}. It follows the sequential importance sampling principle -- in each iteration the particles with higher weights are duplicated while the particles with lower weights are discarded \cite{doucet2001sequential}. This enables higher resolution sampling of the fitness function only in the parts which are likely to contain the (local) optimum. The BPF iteratively performs four main steps -- \textit{resampling}, \textit{prediction}, \textit{update} and \textit{weights normalization}. The detailed breakdown of all steps is depicted in Algorithm \ref{alg:tracking}. Note that the function $predict()$ in \textit{update} step should correspond to the required motion model of the tracked object (a normal distribution $\mathcal{N}(\mu, \sigma)$ is given as an example) and it can be designed to allow for the rapid camera motion which suits the needs of OLS. Therefore, the tracker based on BPF is utilized (see Section \ref{txt:bgfg_tracker}).

\begin{algorithm}
	\SetAlgoNoLine	
	\KwIn{A measurements $M$, a set of particles $P = \{\particle{t}{1}, \particle{t}{2}, ..., \particle{t}{n_{p}}\}$}
	\KwOut{The particle $\particle{t}{i_{bestParticle}}$ representing the state estimation with best weight $w_{t}^{i}$}
	\DontPrintSemicolon
	\SetCommentSty{textsc}
	
	\BlankLine
	
	\tcc{Resampling (importance sampling)}
	$resampleParticles()$\;
	
	\BlankLine
	\tcc{Prediction}
	\ForEach{$\particle{t}{i} \in P$}
	{
		\ForEach{parameter $k_{j}$ of ${x_{t}^{i}}$}
		{		
			$predict(k_{j})$ /* (e.g. $k_{j} = k_{j} + x, ~ x \sim \mathcal{N}(\mu_{j}, \sigma_{j})$) */ \;
		}
	}
	
	\BlankLine
	\tcc{Update}
	\ForEach{$\particle{t}{i} \in P$}
	{
		$w_{t}^{i} = fitnessFunction(M, \particle{t}{i})$\;
	}
	
	\BlankLine
	\tcc{Estimate final state e.g. using MAP}
	$\particle{t}{i_{bestParticle}} = \particle{t}{i} \in P : !\exists ~ \particle{t}{j} \in P : w_{t}^{j} > w_{t}^{i}$\;
	
	\BlankLine	
	\tcc{Weights normalization}
	\ForEach{$\particle{t}{i} \in P$}
	{
		$w_{t}^{i} = \frac{w_{t}^{i}}{\sum_{j = 1}^{n_{p}}w_{t}^{j}}$
	}
	
	
	\caption{Tracking using BPF}
	\label{alg:tracking}
\end{algorithm}

%.........................................................................
%.........................................................................
\subsection{Tracking-Learning-Detection} \label{txt:tracking_learning_detection}

The Tracking Learning Detection (TLD) \cite{Kalal:2012:TRA:2225045.2225082} is an algorithm designed for performing so called long-term tracking -- a robust tracking of an object which might change its appearance, be temporarily occluded by closer objects or temporarily completely disappear from the scene. TLD is based on the appearance representation of the target, specifically a set of templates is stored and continuously updated. Additionally, TLD can be implemented to run in real-time. Such properties meet the requirements of OLS (see Section \ref{txt:introduction}, object tracking), thus TLD is incorporated in OLS as one of the visual trackers and this section explains its design in more detail.

Since a long-term tracking cannot be easily achieved neither by a mere tracker nor by a detector, the TLD aims to combine the strengths of the detection and tracking algorithms by combining their results. Furthermore the algorithm incorporates the online adaptation subsystem capable of learning the new appearances of the tracked object in the course of the tracking.

A conceptual diagram of the TLD algorithm is shown in Figure \ref{fig:tld_block_diagram}. The \texttt{tracking} component tracks the object and for each frame produces the new position. It expects that object does not disappear (occlusion, out of FOV) from the scene and if it does, the tracker fails. The \texttt{detection} component performs full scanning of the image for each frame. It detects the object and if needed it reinitializes the tracker. The \texttt{learning} component is capable of generating new appearances of the tracked object and thus improving the performance of the detector. 

The object itself is modeled as a set of patches, each patch being already learned appearance represented by the rectangular bounding box around the object rescaled to a normalized resolution of 15 x 15 pixels. The similarity of the patches is given by NCC.

%% TLD block diagram
%% TLD PN learning block diagram
\begin{figure}[htb]
	\centering
	\begin{minipage}{.34\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/tld_block_diagram.png}
		\captionof{figure}{A diagram of the main TLD components. Image is adopted from \cite{Kalal:2012:TRA:2225045.2225082}.}
		\label{fig:tld_block_diagram}
	\end{minipage}
	\hfill
	\begin{minipage}{.63\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/tld_pn_learning_block_diagram.png}
		\captionof{figure}{A diagram of the PN learning process. Image is adopted from \cite{Kalal:2012:TRA:2225045.2225082}.}
		\label{fig:tld_pn_learning_block_diagram}
	\end{minipage}
\end{figure}

\paragraph{Detection} Detector treats each frame as an independent one and scans a full image. A scanning window is used and it is gradually scaled (in order for the detector to achieve scale invariance) and iteratively shifted along a regular grid of candidate positions. Since this task is computationally intensive a cascade classifier is used so that the detector could quickly decide whether a given subregion contains the object. In case of TLD, the cascade classifier consists of three sequential stages specifically ordered so that earlier the stage is the more subregions it should reduce while being computationally less expensive. Should the subregion be rejected by any stage, later stages ignore it completely.

\paragraph{Tracking} The tracking subsystem is based on the algorithm called the Median-Flow tracker. A $10 \times 10$ grid is used to select the positions within the bounding box representing the object. For each position a given point is tracked between the consecutive frames using pyramidal Lucas-Kanade tracker and eventually the tracker only accepts a 50\% of the most reliable displacements to estimate a new position of the target.

\paragraph{Learning} Since the classifier used in the detection phase is initially trained using only one positive patch (the initial bounding box selected by a user) it tends to make errors as a video stream progress since the moving object of interest change its appearance due to the transformation caused by its motion. Therefore an online so called \texttt{P-N learning} component is incorporated in the system which gradually extends the training sets. Two experts are used, a \texttt{P-expert} identifies only false negatives while \texttt{N-expert} identifies only false positives. Once a wrongly classified patch is found, the experts extend the training set and the classifier is retrained (see Figure \ref{fig:tld_pn_learning_block_diagram}).

%.........................................................................
%.........................................................................
\subsection{Tracking Using Background/Foreground Modeling and Particle Filter} \label{txt:bgfg_tracker}

The autonomous tracking uses the implementation of the visual tracker combining the background subtraction, motion model and object model in the particle filter framework  proposed in \cite{ObjectTrackinginMonochromaticVideo}. Thanks to both particle filter and inter frames homography computation this approach can even cope with the moving cameras. Partial occlusion is handled using foreground modeling and the tracker is capable of running in real-time. Therefore, this approach si suitable for OLS as well and it is used as an alternative to TLD. The operation of the tracker is described below.

The target is represented as a rectangular template (consisting of gray-scale intensity values), which is normalized to the size $24\times24$ pixels. The template is created only once during the initialization, and thus the tracker could fail if the target changed its appearance significantly during the course of tracking. However, in case of very distant targets only a slight change occurs.

The Bootstrap particle filter (BPF) \cite{Isard98condensation} is used to generate and evaluate candidate positions of the target. Each particle (i.e. the state of the system) is represented as $\vec{x_{n}} = (x, y, v_{x}, v_{y}, h, w)$, where $(x, y)$ represents the 2D position of the target, $(v_{x}, v_{y})$ represents the estimated speed of the target and $(h, w)$ represents the bounding box size. 

The perturbations in the observed position of the target caused by the moving camera are alleviated using the motion model which is applied in the \textit{prediction} step of the BPF:
\begin{align}
	pos_{n+1} &= pos_{n} + vel_{n} + \gamma_{pos} \sim \mathcal{N}(\mu, \sigma),\\
	vel_{n+1} &= vel_{n} + \gamma_{vel} \sim \mathcal{N}(\mu, \sigma),\\
	bb_{n+1} &= bb_{n} + \gamma_{bb} \sim \mathcal{N}(\mu, \sigma),	
\end{align}
where scalar $pos_{n}$ is the $x$ or $y$ position, scalar $vel_{n}$ is the $x$ or $y$ velocity, scalar $bb_{n}$ is the $w$ or $h$ size of the bounding box in time $n$, and $\gamma$ is the noise drawn from the Gaussian distribution $\mathcal{N}(\mu, \sigma)$, where scalars $\mu$ and $\sigma$ parameters are set empirically for each parameter.

In the \textit{update} step, each particle is assigned a new weight $w$ using the objective function reflecting the similarity of the template and the candidate patch, The function serves the purpose of the similarity measure and it is based on a SSD:
\begin{align}
	w = \sum_{(x,y) \in I}{e^{min(M_{t}^{(x,y)}, M_{c}^{(x,y)})}(1 - |I_{t}^{(x,y)} - I_{c}^{(x,y)}|)^{2}},
\end{align}
where $M_{t}$ and $M_{c}$ are the foreground masks (FM) of the template and the current candidate respectively, $I$ is the image, $t, c$ subscripts denote template and candidate patch respectively, and $(x,y)$ superscript denotes indexing 2D array (an image). The FMs are estimated by subtraction of the two images where the bounding boxes denoting the position of the target do not overlap (the FM $M_{t}$ is estimated only once). The resulting estimate of the target position is chosen using the Maximum a posteriori approach.

In order to enable the motion of the camera, the transformation between each pair of adjacent frames is estimated by detecting and tracking the keypoints using KLT tracker \cite{Tomasi91detectionand} and then estimating the homography using the RANSAC algorithm \cite{Hartley:2003:MVG:861369}.

%\section{Motion Predicition and Regulation}
%
%%.........................................................................
%%.........................................................................
%\subsection{2D and 3D Position Prediction}
%\todo{moving average}
%\todo{linearni regrese}
%\todo{RANSAC}
%
%%.........................................................................
%%.........................................................................
%\subsection{Hardware Regulation}
%\todo{PID}
%\todo{funkcni zavislost rychlosti na vzdalenosti cile od stredu}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Multi-Camera Target Localization} \label{txt:multi_camera_target_localization}

This section introduces the problematics of estimating the location of a target in 3-space given the arbitrary number of corresponding image points in 2-space. First the suitable camera model is described, then the standard stereo setups are examined, next the notion of triangulation with noisy measurements is presented and finally the existing approaches to target location estimation as well as their suitability for OLS are discussed.

\subsection{Camera Model}

Localization of a target corresponds to a process of mapping image locations of a target in 2-space from multiple cameras to a location in 3-space. Thus it is necessary to define a model of a camera first. The \textit{Finite pinhole camera} based on the \textit{central projection} is a standard approach to model cameras with CCD like sensors \cite{Hartley:2003:MVG:861369}. and it is used to model hardware cameras in OLS as well. Finite pinhole cameras use projection matrix $P$ which maps a point $\vec{X}$ in 3-space to an image location $\vec{x}$ in 2-space (see Figure \ref{fig:pinhole_camera}):

\begin{align}
	\vec{x} &= P\vec{X},\\
	P &= KR[I|-C],\\
\end{align}
where $I$ is the identity matrix, $R$ and $C$ are the rotation and translation matrices representing the orientation and position of the camera frame with respect to the world frame and $K$ is the \textit{intrinsics matrix} (or \textit{camera calibration matrix}) consisting of a focal length $f$, coordinates of the principal point $PP = (x_{0}, y_{0})^{T}$ and skew parameter $s$:
\begin{align}
	K = \begin{bmatrix}
		\alpha_{x} & s          & x_{0} \\
		0          & \alpha_{y} & y_{0} \\
		0          & 0			& 1
	\end{bmatrix}.
\end{align}

%% Pinhole camera
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.60\textwidth]{fig/pinhole_camera.pdf}
	\caption{Perspective projection of point $T$ in 3-space to point $t$ in 2-space located on the image plane in a finite pinhole camera model.}
	\label{fig:pinhole_camera}
\end{figure}

\subsection{Stereo Setups}

With two calibrated cameras (i.e. $K$, $R$ and $C$ matrices are known) observing the same portion of the environment it is possible to estimate the location of a given point/object in 3-space. The \textit{Canonical stereoscopic system} is one of the widely used setup capable of estimating the depth of points in the scene \cite{Cyganek:2007:ICV:1214366}. The optical axes of both cameras are collinear and the notion of \textit{disparity} is introduced. Disparity refers to the difference in the image location of the same 3D point when projected under perspective to two different cameras \cite{Stockman:2001:CV:558008} and the coordinates of a point in 3-space can derived using following equations (see Figure \ref{fig:canonical_vs_general_stereo}):

\begin{align}
	z &= fb/(x_{l} - x_{r}) = fb/d,\\
	x &= x_{l}z/f = b + x_{r}z/f,\\
	y &= y_{l}z/f = y_{r}z/f,
\end{align}
where $f$ is a focal length, $b$ is the baseline, $x_{l}$ and $x_{r}$ are the horizontal distances between the principal points $PP$ of the respective camera and the projection $t_{l}$ and $t_{r}$ respectively (the same applies for $y_{l}$ and $y_{r}$ in vertical direction), $d$ is the disparity and $(x, y, z)^{T}$ are the 3-space coordinates of the target. Nevertheless, OLS cannot be modeled as canonical stereoscopic system due to the fact that it is designed to work with arbitrary number of cameras and furthermore, the extrinsics of the cameras are not fixed (the cameras rotate freely in space, see Section \ref{txt:system_overview}).

%% Canonical stereo setup vs general stereo setup
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/canonical_stereo_vs_general_stereo.pdf}
	\caption{In canonical stereo setup (left) the 3D coordinates of a given point can be computed using the notion od disparity. On the other hand, in case of general stereo setup (right) the intersection of lines in 3-space defined by the target projected to the image space of each camera must be found.}
	\label{fig:canonical_vs_general_stereo}
\end{figure}

In \textit{general stereo setup} the collinearity of the optical axes is not required. However, location of a point in 3-space cannot be simply estimated through triangle similarity. On each camera a line in 3-space mapping to a point $\vec{p_{i}}$ in 2-space has to be first computed using \textit{back-projection} (see Figure \ref{fig:canonical_vs_general_stereo}):
\begin{align}
	X(\lambda) = P^{+}\vec{x} + \lambda C,
\end{align}
where $P^{+}$ is the pseudo-inverse of $P$ ($P^{+} = P^{T}(PP^{T})^{-1}$). The location of the target in 3-space is then given as an intersection of all back-projected lines. Given its nature, OLS can be modeled as a general stereo setup extended to use arbitrary number of cameras (instead of only two cameras).

\subsection{Triangulation with Noisy Measurements}

The process of finding the location of a target in 3-space as an intersection of back-projected lines is called \textit{triangulation}. In ideal case the projection matrices $P_{i}$ and calibration parameters are known precisely for both cameras and the lines in 3-space intersect. However, in real world the system is subject to both systematic and random error (see Chapter \ref{txt:precision_of_the_localization}), consequently the lines in 3-space might become skew (i.e. they are not guaranteed to intersect anymore) since the stereo system does not satisfy the epipolar constraint \cite{Hartley:2003:MVG:861369} (more detailed explanation can be found in Section \ref{txt:3d_environment_reconstruction}):
\begin{align}
	\vec{x'}^{T}F\vec{x} \neq 0.
\end{align}

In general, the same problem holds within each pair of cameras in N-view setup. Instead of the intersection the minimum distance between each pair of skew lines might be found as a line segment perpendicular to both skew lines (see Figure \ref{fig:multi_view_intersection}) via following equation \cite{Forsyth:2002:CVM:580035}:
\begin{align}
	z_{ij}\vec{p_{i}} &= \vec{T_{ij}} + z_{ij}R_{ij}\vec{p_{j}} + \lambda_{ij}(\vec{p_{i}} \times R_{ij}\vec{p_{j}})\\
	for~i,j           &= 1,2,...,N \land i \neq j,
\end{align}
where $\vec{p_{i}}$ is the direction of back-projected line in camera $i$, $z_{ij}$ gives the scale of vector ${p_{i}}$ so as to define a point $P_{ij}$ which is the closest to the line back-projected from camera $j$, $T_{ij}$ and $R_{ij}$ are the translation and rotation of the coordinate frame of the camera $j$ with respect to the coordinate frame of the camera $i$ and $\lambda_{ij}$ defines the length of the line segment connecting both back-projected skew lines.

A similar approach to finding the closest distance between each pair of cameras is used in OLS (see Section \ref{txt:localization}).

%% Skew lines in multi view scenario
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/multi_view_intersection.pdf}
	\caption{An example of a 3-view setup and the target location estimation using triangulation. Since all cameras are subject to systematic and random error the back-projected lines are skew, thus there is no intersection. The closest distance between each pair of liness is given by the line segment perpendicular to both lines.}
	\label{fig:multi_view_intersection}
\end{figure}

\subsection{Estimation of Target Location}

Since the back-projected lines are skew, there is not the only correct solution to the localization problem. Contrarily, the position of the target must be estimated. Hartley and Zisserman \cite{Hartley:2003:MVG:861369} propose a couple of methods where the approaches used basically boil down either to solving the overdetermined system of linear equations or to minimization of the geometric error. Both approaches are briefly described below.

\paragraph{Direct Linear Transformation (DLT)} The DLT algorithm is based on the assumption that an overdetermined system of linear equations in the form $\matr{A}\vec{x} = \vec{0}$ (where $\matr{A}$ is the matrix of coefficients, $\vec{x}$ is the vector of unknowns and $\vec{0}$ is the zero vector) is available and that given the noise there is no exact solution. In case of a stereo setup where the image points $\vec{t_{1}}$ and $\vec{t_{2}}$ in 2-space on each camera correspond to a target $\vec{T}$ in 3-space the overdetermined system of linear equations can be defined with vector $\vec{x} = (t_{1}^{x}, t_{1}^{y}, t_{2}^{x}, t_{2}^{y})$ and matrix $\matr{A}$:
\begin{align}
	A = \begin{bmatrix}
		t_{1}^{x}\vec{p_{1}^{3}}^{T} - \vec{p_{1}^{1}}^{T} \\
		t_{1}^{y}\vec{p_{1}^{3}}^{T} - \vec{p_{1}^{2}}^{T} \\
		t_{2}^{x}\vec{p_{2}^{3}}^{T} - \vec{p_{2}^{1}}^{T} \\
		t_{2}^{y}\vec{p_{2}^{3}}^{T} - \vec{p_{2}^{2}}^{T} \\
	\end{bmatrix},
\end{align}
where $\vec{p_{cam}^{r}}$ is the $rth$ row of the projection matrix $\matr{P_{cam}}$ of the camera $cam$. The ultimate solution is found using the SVD\footnote{Singular Value Decomposition} as the singular vector corresponding to the smallest singular value of $\matr{A}$.

\paragraph{Reprojection Error Minimization} Similarly as DLT this method assumes that the correspondence between image points $\vec{t_{1}}$ and $\vec{t_{2}}$ does not meet the epipolar constraint. The aim thus is to estimate the position of the target $\vec{T}$ in 3-space which projects to image points $\vec{t_{1}'}$ and $\vec{t_{2}'}$ satisfying the epipolar constraint and which at the same time minimizes the reprojection error function $ref$:
\begin{align}
	ref(\vec{t_{1}}, \vec{t_{2}}) = d(\vec{t_{1}}, \vec{t_{1}'})^{2} + d(\vec{t_{2}}, \vec{t_{2}'})^{2},
\end{align}
where $d(\vec{x}, \vec{x'})$ is the Euclidean distance between the measurement $\vec{x}$ and reprojected image point $\vec{x'}$ (see Figure \ref{fig:reprojection}). The reprojection error function can be either minimized using any optimization method or the minimum can even be found non-iteratively in closed form.

%% Reprojection
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.6\textwidth]{fig/reprojection.pdf}
	\caption{The demonstration of reprojection error in a stereo setup where $e_{i}$ is the epipole, $el_{i}$ is current estimation of epipolar line and $d_{i}$ is the Euclidean distance between the initial noisy measurement $t_{i}$ and the reprojection of the target estimate $t_{i}'$ in the camera $i$.}
	\label{fig:reprojection}
\end{figure}

Even though both DLT and reprojection error minimization could be extended so as to support multiple-view setup, neither of these approaches is suitable for OLS since they do not consider any a priori known information about the reliability of the back-projected line in each camera. It has been shown that in OLS the precision of target location estimation strongly depends on the mutual position of the target and the baseline of given camera pair (see Chapter \ref{txt:precision_of_the_localization}). Furthermore, it is possible to obtain the belief from individual visual trackers (i.e. the confidence that the object is tracked correctly). In order to exploit these prior information a specific location estimation method was proposed for OLS (see Section~\ref{txt:localization}).

%\subsection{Existing Multi-Camera Localization Systems}
%
%Professional systems aiming on automatic localization of aerial (as well as terrestrial and underwater) targets mostly rely on active devices, such as radars or sonars and there is very little use of pure passive optical devices (such as RGB cameras, thermal imaging cameras, etc.). The reason might be the complexity of the whole solution or the operation constrained by the weather conditions.
%
%On the other hand since the application of multiple view geometry has been one of hot topics in the computer vision during the past decade, many R\&D groups (specializing mostly on robotics) attempt to base their solutions on multi-camera optical localization.
%
%Professional systems aiming on automatic localization of aerial, terrestrial and/or underwater targets mostly rely on active devices, such as radars or sonars and there is very little use of pure passive optical devices (such as RGB cameras, thermal imaging cameras, etc.). As of today the multi-camera optical localization systems are mostly used for R\&D purposes and mainly in robotics. A common approaches is to set up so called \textit{intelligent space} \cite{intelligentSpace}, the bounded area under surveillance of multiple cameras reporting to the the central system. 
%
%In \cite{Multi-Camera_Sensor_System_for_3D_Segmentation} multiple terrestrial robots are detected, tracked and localized by the multi-camera system. For all cameras, the intrinsics and extrinsics are known beforehand and do not change over time.  A similar approach is used also in \cite{Localization_and_Geometric_Reconstruction_of_Mobile_Robots} but this system uses the robot's odometers as well in order to improve the resulting position estimated by the optical system. The system presented in \cite{A_3D_visual_localization} then relies on four ordinary RGB cameras with known parameters (intrinsics, extrinsics) to estimate the position of the easy to detect and static object using the Perpendicular Foot Method algorithm.
%
%Even though all of the aforementioned approaches utilize the ordinary RGB cameras for detection and tracking and in general estimate the location of the object using the triangulation algorithm, they all assume the object moves strictly within the specified bounded region and they rely on fixed positions of the cameras. Thus, they do not have to deal with the imprecisions arose from the uncertainty of the current camera pose estimation and with the problems of handing off the target to other camera units once a target moves out of the operational range of the given camera. 

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{3D Environment Reconstruction} \label{txt:3d_environment_reconstruction}

One of the most challenging and still not fully solved problems of visual tracking algorithms is the occlusion \cite{Zhang:2010:RVT:1937728.1937766, conf/iccv/KwakNHH11}, i.e. the case where the tracked object becomes partially or fully overlapped by another object. Even though both visual trackers the OLS is based on (see Sections \ref{txt:tracking_learning_detection} and \ref{txt:bgfg_tracker}) are robust against partial occlusion and to the limited extent even against full occlusion, it is necessary to detect the occlusion using the visual clues which might not be reliable. 

However, if the 3D model of the surrounding scene is known and motion model in 3-space of the tracked target is estimated, it is possible to predict both start and end of full occlusion occurrence in advance and consequently to adjust the tracking algorithm temporarily so that it would not fail. If the 3D model of the environment is not known beforehand it can be reconstructed using mere visual information obtained from cameras.

Therefore, this section introduces the problematics of 3D reconstruction using visual cues in multi-camera system, namely the basics of epipolar geometry, the notion of bundle adjustment and a popular tool for performing both sparse and dense reconstruction in multiple-camera setup -- VisualSFM. A proposed algorithm of occlusion prediction based on the knowledge of sparse 3D model of the environment is presented in Section \ref{txt:occlusion_prediction}. 

%.........................................................................
%.........................................................................
\subsection{Reconstruction Pipeline} \label{txt:reconstruction_pipeline}

Assuming the most general scenario where multiple images of the scene taken from multiple uncalibrated cameras are available, a 3D reconstruction pipeline based on the \textit{iterative bundle adjustment} can be utilized \cite{Snavely:2006:PTE:1179352.1141964} (see Figure \ref{fig:3d_reconstruction_pipeline}).

%% 3D reconstruction pipeline
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.8\textwidth]{fig/3d_reconstruction_pipeline.pdf}
	\caption{A 3D reconstruction pipeline taking multiple images taken from uncalibrated cameras and producing estimated camera parameters and locations of matched points in 3-space.}
	\label{fig:3d_reconstruction_pipeline}
\end{figure}

The algorithm takes arbitrary number of images on the input performs \textit{SIFT keypoint detection} as the first step. Next, individual keypoints are matched across all images creating the \textit{tracks}. A \textit{fundamental matrix $\matr{F}$} is estimated for each pair of images (see Section \ref{txt:epipolar_geometry}) and those matches which are outliers with regards to $\matr{F}$ are removed. Finally, the \textit{iterative sparse bundle adjustment} is run (see Section \ref{txt:bundle_adjustment}) which produces the estimates of camera parameters (both intrinsics and extrinsics) and 3D locations of matched points.

In case of OLS both extrinsics and intrinsics are known. However, they are correct only up to a systematic and random error caused by imprecise stationing and rectification (see Chapter \ref{txt:precision_of_the_localization}) and a noise which the P\&T orientation measurements are subject to. Therefore in order to achieve a 3D reconstruction of the surrounding environment it is still reasonable to employ the bundle adjustment technique. In order to exploit the a priori known information, for instance the bundle adjustment might be initialized with known camera calibration and pose parameters so as to make the optimization algorithm more likely to find the global optimum.

%.........................................................................
%.........................................................................
\subsection{Epipolar Geometry} \label{txt:epipolar_geometry}

Epipolar geometry represents the relation between two projective pinhole cameras observing the same point (points) in 3-space \cite{Cyganek:2007:ICV:1214366} (see Figure \ref{fig:epipolar_geometry}). The line between both camera centers $\vec{C_{i}}$ is called the \textit{baseline} and it delimits the \textit{epipole} $ \vec{e_{i}}$ in each projective plane $Im_{i}$. The projection of target $\vec{T}$ to both image planes defines image points $\vec{t_{i}}$ which in return backproject to lines in 3-space. The line $el_{i}$ lying in image plane $Im_{i}$ connecting $\vec{e_{i}}$ and ${\vec{t_{i}}}$ is called \textit{epipolar line}.

What the epipolar geometry relation says is that the observed target $\vec{T}$ must lie only in the plane $Ep$ defined by the baseline and both backprojected lines (from camera center $\vec{C_{i}}$ through image point $\vec{t_{i}}$). Alternatively, the epipolar line $el_{i}$ is the projection of back-projected line $\vec{C_{j}}\vec{t_{j}}$ to the projection plane $Im_{i}$.

%% Epipolar geometry
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.7\textwidth]{fig/epipolar_geometry.pdf}
	\caption{The epipolar geometry.}
	\label{fig:epipolar_geometry}
\end{figure}

Ultimately, the \textit{epipolar constraint} is defined as following: \textit{Each image point $\vec{t_{i}}$ of a space point $\vec{T}$ lies in the image plane only on the corresponding epipolar line}. This can be stated numerically using the \textit{fundamental matrix} $\matr{F}$:
\begin{align}
	\vec{t_{1}}^{T}\matr{F}\vec{t_{2}} = 0
\end{align}

Fundamental matrix $\matr{F}$ can be estimated from mere image correspondences for instance using normalized 8-point algorithm within RANSAC framework \cite{Hartley:2003:MVG:861369, Cyganek:2007:ICV:1214366}.

%.........................................................................
%.........................................................................
\subsection{Bundle Adjustment} \label{txt:bundle_adjustment}

Bundle adjustment is the approach which aims to simultaneously refine the parameters of all involved cameras (intrinsics and extrinsics) and to minimize the reprojection error between the initially measured image point and reprojected target. The non-linear least squares error function $E$ can be defined as \cite{Forsyth:2002:CVM:580035}:
\begin{align}
	E = \frac{1}{mn}\sum_{i,j}{[(x_{ij} - \frac{\vec{p_{i1}}\vec{T_{j}}}{\vec{p_{i3}}\vec{T_{j}}})^{2} + (y_{ij} - \frac{\vec{p_{i2}}\vec{T_{j}}}{\vec{p_{i3}}\vec{T_{j}}})^{2}]},
\end{align}
where $m$ is the number of cameras, $n$ is the number of target points in 3-space, $(x_{ij}, y_{ij})$ is the initially measured location of the projection of target $T_{j}$ to the projective plane of camera $i$ and $\vec{p_{ir}}$ is the rth row of projection matrix $P_{i}$. As for the minimization, \textit{Levenberg-Marquardt} algorithm is mostly used. 

In case of reconstruction approach proposed by Snavely, Seitz and Szeliski \cite{Snavely:2006:PTE:1179352.1141964} (see Section \ref{txt:reconstruction_pipeline}) the iterative version of bundle adjustment is used. In this case a most suitable image pair, which has enough matches and large baseline is selected and the camera parameters as well as the 3D locations of matched points are estimated. In each next iteration a new camera is added to the optimization algorithm.

%.........................................................................
%.........................................................................
\subsection{VisualSFM}

VisualSFM\footnote{Official website of VisualSFM: \url{http://ccwu.me/vsfm/index.html}} is a well established application for end-to-end scene reconstruction using multiple cameras which follows the standard pipeline described in Section \ref{fig:3d_reconstruction_pipeline} and adds another method for dense reconstruction. For sparse reconstruction the application uses parallel implementation of bundle adjustment \cite{ChangchangWu:2011:MBA:2191740.2191945} and for dense reconstruction the Patch-based Multi-view Stereo Software (PMVS) approach is utilized \cite{Furu:2010:PMVS}. The demonstration of dense reconstruction is depicted in Figure \ref{fig:visualsfm_tower}. Upon successful reconstruction the VisualSFM enables the 3D point cloud to be exported in standard PLY format which can be easily imported to OLS for instance with the use of Point Cloud Library (see Section \ref{txt:occlusion_prediction}).

%% Visual SFM tower
\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.9\textwidth]{fig/visualsfm_tower.png}
	\caption{The 3D reconstruction performed by VisualSFM from only two photographs taken by the same camera in neighborhood Štýřice, Brno. Note that even mere two views suffice to reconstruct the church tower, however, the output data include a lot of noise.}
	\label{fig:visualsfm_tower}
\end{figure}

%=========================================================================
%=========================================================================
\chapter{Precision of the Localization in Multi Camera System} \label{txt:precision_of_the_localization}
\todo{Rozdelit typy chyb}
\todo{Error map - zavislost chyby lokalizace na vrcholovem uhlu a velikosti baze a poctu jednotek}
\todo{Zavislost presnosti na chybe trackeru v px - v simulatoru}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{System Topology}

The main building block of the OLS system is a \texttt{camera station} (CS). CS consists of the \texttt{camera unit} (CU), which is a collection of hardware necessary for capturing the images, manipulating the position of the camera and estimating the geographical coordinates of the CS (for detailed description see Chapter \ref{txt:camera_unit}), and the computation unit processing the data (a PC or other device). In general, the OLS is designed to work with an arbitrary number of CSs (of course, there must be at least two stations).

As for the network topology, the OLS is based on a star pattern where each tracking station communicates only with the overview station (see Section \ref{txt:hardware}). The positional organization of the CSs depends on the number of tracking stations, which should always be placed so that their positions projected to the horizontal plane would form a regular polygon with the overview unit in the center (see Figure \ref{fig:system_overview}), however, in more complex environments this condition does not have to strictly hold (see Figure \ref{fig:spilberk_camera_units}).

%% Schema of camera units organized in triangle.
%% Use case of camera units - placed on map of Brno's Spilberk
\begin{figure}[htb]
	\centering
	\begin{minipage}{.45\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/system_overview.pdf}
		\captionof{figure}{Schematic view of the ideal-case organization of all camera units where all three tracking units make up an equilateral triangle with the overview unit in the middle. The initial orientation of the tracking units is shown as well as their FOVs $\gamma_{1-3} = 38^{\circ}$ (assuming camera Prosilica GT 1290C and a lens with equivalent 50mm focal length).}
		\label{fig:system_overview}
	\end{minipage}
	\hfill
	\begin{minipage}{.48\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/spilberk_camera_units.pdf}
		\captionof{figure}{Use case scenario showing the organization of the camera units within the system set to protect a real world area (the castle Spilberk in Brno, Czech Republic). As can be seen in the Figure, given the possibilities of the protected area (some CUs mounted on the rooftops) the CUs do not form an equilateral triangle, neither is the overview unit positioned in the middle.}
		\label{fig:spilberk_camera_units}
	\end{minipage}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Precision Analysis}


%=========================================================================
%=========================================================================
%\chapter{Sensitivity analysis} \label{txt:sensitivity_analysis}
%The precision of the system can be defined in the means of the frame-by-frame Euclidean distance between the estimated location and the real (ground truth) location of the given target. The precision is impacted by multiple independent factors, thus it is essential to perform the sensitivity analysis in order to discover and prospectively alleviate the most prominent contributors of the overall error. 
%
%- vysvetleni typu chyb:
%	- system error (systemova chyba)
%		- nesoulad modelu CU s realnou konstrukci
%		- nespravne mereni heading
%		- detekce a tracking ?
%	- uncertainty of the input of the system
%		- GPS mereni
%		- data inklinometru
%	
%- vysvetlit, ze se budeme snazit potlacit jen nektere chyby (neresime treba nepresnost mezi modelem a realnou CU z hledsiak translaci mezi klouby)
%
%- rozdeleni na chybu rotace a translace
%	- nepresnost v rotaci je daleko zavaznejsi, nez nepresnost v translaci
%	- priklad a obrazek vlivu nepresnosti o x mrad na lokalizaci cile ve vzdalenosti y m/km
%
%- zdroj chyb:
%	- detekce
%	- tracking
%	- GPS pozice
%	- natoceni vuci severu
%	- rozliseni PTU
%	- model PTU - translace mezi klouby
%	- model PTU - rotace mezi klouby
%	- uchyceni kamery
%		- rotace podle opticke osy
%		- rotace podle osy azimutu
%		- rotace podle osy elevace

%- vysvetleni input a output systemu a pojmu sensitivity analysis (http://samples.sainsburysebooks.co.uk/9780470725177_sample_389211.pdf)

\vata[20]

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Stationing} \label{txt:stationing}

The precision of the whole system is dependent on the uncertainty of the system input as well as on the imprecision of the camera unit construction. The process of stationing aims to alleviate the uncertainty of the system input while the main purpose of the rectification is to reduce the difference between the real camera unit and its model.

Since the stationing is considered to be already a working subsystem of the whole project (and thus is not dealt with within the scope of this work) only the main principle will be briefly described. The stationing is composed of two parts: finding the geographical north and finding the relative azimuthal and elevation angles between each pair of camera units.

%.........................................................................
%.........................................................................
\subsection{Geographical North} \label{txt:geographical_north}

Though it is a common practice to estimate the heading\footnote{Heading is the term used to describe the angle between the torso of the human body and the geographical north \cite{Henriksson648760}} using a magnetometer, this device is unsuitable for this project since the accuracy of the current professional class magnetometers is insufficient. For instance the accuracy of the magnetometers meant for compassing applications produced by Honeywell company range from hundreds to thousands of milliradians \cite{Honeywell:compassing_catalog}.

In order to find the orientation of each camera unit placed in the outdoor environment, distinctive landmarks (created either by human or nature) with known geographical positions are used. For each such landmark, the manipulator is rotated so that the optical axis of the camera would intersect that landmark and both the azimuth and elevation value is registered. Using triangulation the geographical position of the camera unit is derived. 

A different possible approach takes advantage of the celestial objects, such as the moon, sun or stars for which the current geographical position is known as well. Nevertheless, this approach can only be used between the sunset and the dawn.

%.........................................................................
%.........................................................................
\subsection{Relative Azimuth and Elevation}

To further reduce the impact of the uncertainty of the system input produced by the GPS and the system error given by the imprecision of the heading estimation (see Section \ref{txt:geographical_north}) it is convenient to find the relative position of each camera unit with regards to the rest of the camera units.

The information about the geographical position of all camera units as obtained from the GPS sensors is distributed across the whole system. Each pair of camera units then automatically performs the following:

\begin{enumerate}
	\item Set the azimuth and elevation of the manipulator so that the optical axis of the camera would intersect the expected location of the LED target of the other camera unit.
	\item Using the visual clue adjust the azimuth and elevation so that the optical axis of the camera would intersect the center of the LED target of the other unit (see Figure~\ref{fig:stationing_aiming}).
	\item Save the current azimuth and elevation values of both camera units and use those values to update the model of the system.
\end{enumerate}

%% Stationing process of one pair of the camera units
\begin{figure}[htb]
	\centering
	\includegraphics[width=8cm]{fig/stationing_aiming.png}
	\caption{Schema of the stationing process where two camera units attempt to align the optical axes of their cameras so that they would intersect the LED target of the other unit.}
	\label{fig:stationing_aiming}
\end{figure}


%.........................................................................
%.........................................................................
\subsection{Horizontality}
Since the camera unit is expected to be placed in an unknown outdoor terrain, it will never stand on an ideally horizontal surface. Therefore, it is necessary to either ensure that the unevenness of the surface is compensated by the suitable setting of the camera unit's stand or both the side tilt and front tilt angles of the stand must be estimated and integrated to the model of the given camera unit. For these purposes the inclinometer attached to the base plane of the camera unit (see Section \ref{txt:camera_unit}) is used.


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Rectification} \label{txt:rectification}

The process of rectification serves the purpose of reducing the system error caused by the imprecise attachment of the camera to the manipulator. The model of the camera unit assumes that the camera is precisely attached to the manipulator so that the camera image sensor is positioned perpendicular to the azimuthal axis and the rows of the image sensor are parallel to the elevation axis (i.e. the camera is not rotated along the optical axis).

Regarding these requirements the rectification consists of three parts: rotation along the optical axis, rotation along the azimuthal axis, finding the default elevation angle.


%.........................................................................
%.........................................................................
\subsection{Rotation Along the Optical Axis}

A custom made metal mount is attached to the bottom side of the camera. The mount is then attached to the manipulator using two opposing round tenons enabling for the rotation of the mount (together with camera) along the axis parallel to the optical axis of the camera (see Figure \ref{fig:rect_model_front_view}).

%% The front view of the model of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=10cm]{fig/rect_model_front_view.png}
	\caption{Front view of the top part of the camera unit. The red arrow shows the possible rotation of the camera along the axis parallel to the optical axis. The Figure is adopted from company Oprox, a.s.}
	\label{fig:rect_model_front_view}
\end{figure}

In this part the rectification target with three parallel horizontal black lines is used. Each line has a different width so that the operator can select the most suitable one (given the distance of the target, ambient lighting conditions, etc.). As the first step a surveying automatic level is used to rotate the target so that the black lines become horizontal. Then the camera is pointed approximately to the center of the target. The camera image stream is blended with the same stream mirrored across the vertical axis. The operator then manually rotates the camera so that the black lines in this blended image stream appear visually aligned (see Figure \ref{fig:rect_mirrored_stream}). Once set, the mount with the camera is fixed to the manipulator using two set screws.

%% Rectification of the rows of the camera image sensor - original stream blended with the mirrored stream
\begin{figure}[htb]
	\centering
	\includegraphics[width=12cm]{fig/rect_mirrored_stream.png}
	\caption{A blended image stream from the camera before (left) and after (right) rotating the camera along the optical axis to the correct position.}
	\label{fig:rect_mirrored_stream}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Rotation Along the Azimuthal Axis}

The mount can still rotate along the axis parallel to the azimuthal axis (see Figure \ref{fig:rect_model_top_view}). It is necessary to ensure that the optical axis of the camera is perpendicular to the elevation axis. 

%% The top view of the model of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=11cm]{fig/rect_model_top_view.png}
	\caption{Top view of the top part of the camera unit. The red arrow shows the possible rotation of the camera along the axis parallel to the azimuthal axis. The Figure is adopted from company Oprox, a.s.}
	\label{fig:rect_model_top_view}
\end{figure}

The same target from the first part of the rectification is used, but two black crosses are added to the selected horizontal black line. The distance $d_{ao}\ m$ between two crosses equals to the distance between the azimuthal and optical axis (which is known from the engineering design, see Figure \ref{fig:rect_azi_axis}). 

%% Photograph of the camera unit with the telescope mounted on the top
%% Screenshot of the digital crosshair and the offset
\begin{figure}[htb]
	\centering
	\begin{minipage}{.37\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/rect_telescope.png}
		\captionof{figure}{A telescope mounted on top of the manipulator. A person looking through a telescope sees a crosshair.}
		\label{fig:rect_telescope}
	\end{minipage}
	\hfill
	\begin{minipage}{.59\textwidth}
		\centering
		\includegraphics[width=.99\linewidth]{fig/rect_azi_axis.pdf}
		\captionof{figure}{Rectification target with the pairs of black crosses. The two crosses in a pair are $d_{ao}\ m$ appart. A digital corsshair is displayed in order to find the horizontal offset $d_{h}$.}
		\label{fig:rect_azi_axis}
	\end{minipage}
\end{figure}

A military optical monocular telescope (see Figure \ref{fig:rect_telescope}) is mounted on top of the manipulator. The optical axis of the telescope intersects the azimuthal axis, it is perpendicular to it and it intersects the left cross of a given pair on the rectification target. The camera is rotated so that its optical axis (represented by the digital crosshair) intersects the right cross on the target and then it is fixed using set screws. As the screws are tightened the camera is unintentionally rotated a bit again which causes the visual offset between the crosshair and the cross on the target. The offset expressed in pixels is recorded and transformed to the default angle $\beta$ expressed in milliradians (see Figure \ref{fig:rect_pixel_offset}) of rotation along Z-axis of the joint \texttt{camera} in the camera unit model (see Section \ref{txt:camera_unit}):

\begin{equation*}
	\begin{aligned}
		\beta &= \arccos\frac{focal\_length}{offset}, \\
		offset &= pixel\_offset * pixel\_size
	\end{aligned}
\end{equation*}

%% Geometry schema showing how to calculate angle beta - pixel offset
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_pixel_offset.pdf}
	\caption{The top view schema of a rectification target being projected to the image sensor of the camera. The value of the angle $\beta$ is one of the output of the rectification process.}
	\label{fig:rect_pixel_offset}
\end{figure}


%.........................................................................
%.........................................................................
\subsection{Finding the Default Elevation Angle}

Given the application of the system the camera is expected to mainly observe the sky. Considering the limited elevation range of the manipulator \texttt{Flir PTU D46-70} (see Section \ref{txt:camera_unit}), the camera must be mounted to the manipulator with the default elevation angle approximately $-60^{\circ}$. However, after fixing the camera it is necessary to find default elevation angle precisely. 

For this purpose a pair of rectification targets, which consist of vertical black and white lines representing the marks of a ruler. The targets are positioned in a row with the distance of a few meters so that the front target would overlap approximately half of the rare target when observed from the camera. Both targets must be rotated so that the lines become horizontal, then the operator manually adjusts the elevation of the manipulator so that the digital crosshair would intersect the same mark on both targets where the two marks form a straight line (see figure \ref{fig:rect_default_elevation_angle}). Once found the current elevation angle is recorded and integrated to the model of the camera unit as an angle of rotation along the Y-axis of the joint \texttt{camera} (see Section \ref{txt:camera_unit}).

%% The rectification target for finding the default elevation angle.
\begin{figure}[htb]
	\centering
	\includegraphics[width=13cm]{fig/rect_default_elevation_angle.pdf}
	\caption{Front target of a pair of the rectification targets used to find a default elevation angle (left). A screenshot from the image stream of the camera with the crosshair focused on a row where the marks of the rulers align (right).}
	\label{fig:rect_default_elevation_angle}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Camera Calibration}


%=========================================================================
%=========================================================================
\chapter{Design of the Optical Localisation System} \label{txt:system_overview}
\todo{prepsat uvod kapitoly}

This chapter presents the main hardware building blocks of the OLS, the camera stations, and describes the basic pipeline which the system must follow in order to localize a given object. 

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Camera Unit} \label{txt:camera_unit}

The main building block of the OLS is a camera unit (CU) consisting of hardware modules necessary for capturing the images, manipulating the pose of the camera, estimating absolute geographical position and orientation of the station as well as relative position and orientation with regards to the rest of the stations and running the OLS software.

There are two type of CUs. The \textit{overview unit} is designed to be controlled manually by the human operator and is equipped with the zooming lens that allows achieving both a wider scanning range and a more detailed view of the farther objects. The \textit{tracking unit} consists of the fixed lens and takes part in the autonomous tracking of the moving objects and continuous reporting of the estimated directions towards the target.

The hardware components used for tracking stations are described in greater detail in Section \ref{txt:devices}. In order to triangulate the target, the 3D location of the camera as well as the direction of the optical axis must be known for each captured frame, thus a suitable model corresponding to the real hardware mus be designed. Proposed model based on the kinematic chain is introduced in Section \ref{txt:model}.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\subsection{Devices and Components} \label{txt:devices}

A CU (see Figure \ref{fig:camera_unit_photo_model}) consists of a surveying tripod providing a solid base on which a manipulator (P\&T unit\footnote{From English Pan and Tilt.}), a camera and all of the devices used for stationing (LED target, GPS sensor and inclinometer) are mounted. Each CU is equipped with its own desktop computer for processing the image data and calculating the 3D position estimates.

%% A photo of a camera unit and an rviz model.
\begin{figure}[tbh]
	\centering
	\includegraphics[width=10cm]{fig/camera_unit_photo_model.jpg}
	\caption{A photograph (left) of the upper part of the camera unit consisting of a manipulator Flir PTU-D46-70 with the aluminum mount carrying a camera Prosilica GT 1290C and a corresponding 3D model (right) created for \textit{rviz} and \textit{Gazebo} simulator (see Section~\ref{txt:application_of_gazebo}).}
	\label{fig:camera_unit_photo_model}
\end{figure}

\paragraph{Manipulator Flir PTU-D46-70} A manipulator PTU D46-70\footnote{Website of product Flir PTU-D46-70: \url{http://www.flir.com/mcs/view/?id=53712}} produced by manufacturer Flir is used (see Figure \ref{fig:prosilica_gt1290c_flir_ptud4670}). As compared to the other professional manipulators this is a lower middle class device consisting of two stepper motors (pan and tilt axes). The device is capable of maximal angular speed of $60^{\circ}/s$ with the resolution of $0.003^{\circ}$ while the payload must not exceed $4.08~kg$ \cite{Flir_ptud4670}. The operational range is limited to $[-180^{\circ}, 180^{\circ}]$ in azimuth and $[-47^{\circ}, 80^{\circ}]$ in elevation. The manipulator incorporates no position feedback, the position is inferred from the number of steps and the current resolution, thus it is necessary not to overload the manipulator, otherwise it could loose synchrony and report wrong position.

\paragraph{Camera Prosilica GT 1290C} Prosilica GT 1290C\footnote{Website of product Prosilica GT 1290C: \url{https://www.alliedvision.com/en/products/cameras/detail/1290-1.html}} is an industrial camera manufactured by the company Allied Vision (see Figure \ref{fig:prosilica_gt1290c_flir_ptud4670}). It is an RGB camera equipped with CCD sensor (type 1/3'') with the resolution of $1280 \times 960$ px and support for $33.3$ FPS and it communicates through gigabit Ethernet \cite{Prosilica_gt1290c}. What is important, the camera natively supports the Precision Time Protocol (PTP) for precise time synchronization which is a crucial feature in each application relying on stereo vision as it is capable of time synchronization among devices within the range of nanoseconds \cite{PTP}. The manufacturer claims that Prosilica GT 1290C achieves the synchronization precision of $1~\mu s$.

\paragraph{Lense Computar M5018-MP2} Each camera mounted on a tracking unit is equipped with a fixed-focus lense Computar M5018-MP2\footnote{Webiste of product Computar M5018-MP2: \url{http://computar.com/product/556/M5018-MP2}} (see Figure \ref{fig:prosilica_gt1290c_flir_ptud4670}). The focal length $50~mm$ was chosen as a most suitable trade-off between the wide field of view and capability of imaging distant targets. Given the camera sensor type 1/3'' the effective horizontal field of view $fov_{h}$ is approximately $5.5^{\circ}$.

%% Photos of Flir and Prosilica.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{fig/prosilica_gt1290c_flir_ptud4670_computar.png}
	\caption{Product pictures of manipulator Flir PTU D46-70 (left), camera Prosilica GT 1290C (middle) and lense Computar M5018-MP2 (right).}
	\label{fig:prosilica_gt1290c_flir_ptud4670}
\end{figure}

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\subsection{Model} \label{txt:model}

The model of the camera unit is based on the kinematic chain consisting of six joints and five links corresponding to the distance between separate parts of the surveying tripod and separate parts of the manipulator (see Figure \ref{fig:camera_unit_kinematic_chain}). The starting joint \textsc{ground} itself is dependent on the reference location (let us call it \textsc{origin}) which represents the origin of the global coordinate frame. The transformation between \textsc{origin} and \textsc{ground} reflects the position and orientation of the given manipulator within the environment (which is estimated during the stationing process, see Section \ref{txt:stationing}).

The kinematic chain is designed as the composition of transformation matrices where a single joint can be located as a solution to the \textit{forward kinematics problem}, i.e. by applying the Euclidean transformation on the position of the joint it is dependent on. For instance the transformation matrix $M_{cam}$ of the joint \textsc{camera} can be derived as:

\begin{equation}
M_{cam} = M_{ele}T_{cam}R_{Z_{cam}}R_{X_{cam}}R_{Y_{cam}},
\end{equation}
where $M_{ele}$ is the transformation matrix of the joint \textsc{ele} which the joint \textsc{camera} is dependent on, and $T_{cam}$ and $R_{AXIS_{cam}}$ are transformation and rotation matrices describing transformation from the joint \textsc{ele} to the joint \textsc{camera}. 

Note that from the implementation point of view, the OLS is built on the ROS framework (see Chapter \ref{txt:implementation}) which presents certain conventions, most importantly the orientation of the coordinate frame which will is used throughout the document (see Figure \ref{fig:frame_convention}).

%% ROS frame conventions
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.2\linewidth]{fig/frame_convention.pdf}
	\caption{Frame orientation convention which is used throughout the work --- a right handed coordinate system with X axis aiming forward, Y axis aiming left and Z axis aiming up.}
	\label{fig:frame_convention}
\end{figure}

%% A schema of the camera unit - kinematic chain.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\linewidth]{fig/camera_unit_kinematic_chain.pdf}
	\caption{Schematic view of the kinematic chain of a camera unit with the joints depicted by the yellow circles with black crosses. The sizes of all components necessary to specify the translation matrices between consecutive joints are shown as well. Note that this is the rear view, i.e. the camera is seen from behind. Thus the joints \texttt{camera} and \texttt{focus} overlap as they both lie on the optical axis (the joint \texttt{focus} has a lower value of the Z coordinate).}
	\label{fig:camera_unit_kinematic_chain}
\end{figure}


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Visual Tracking}

\todo{tracking}
\todo{occlusion prediciton using 3D reconstruction}
\todo{regulation - ukazat nekolik pouzitych funkci pro regulaci (linearni / kvadraticka / exp / ...)}

The homography might not be found, which often occurs if the airborne target with the uniform background of the sky is tracked or if the manipulator moves the camera too harshly. To deal with such cases in OLS, the tracker was adjusted so that in a \textit{prediction} step of the BPF a small subset of particles would be forced to take the image positions yielding the highest response of adjacent frame differencing which is expected to contain the moving target of interest.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Target Localization Using Triangulation} \label{txt:localization}

In OLS, the intrinsics were estimated for each camera during calibration and extrinsics are known at each time due to the sensory data streamed from the manipulators.

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Occlusion Prediction} \label{txt:occlusion_prediction}
- zminit, ze byl testovan widely used SW VisualSFM, ale bohuzel nepodporuje inicialni vlozeni intrinsics a extrinsics pro vice kamer
- zminit, ze je pomoci VisualSFM aspon ukazano, jaky je rozdil mezi pouzitim 2 nebo 4 (ci vice) views na rekonstrukci popelnice z gazeba.
- zminit, ze detailni zabyvani se 3d rekonstrukci je out of scope teto prace, prace se zameruje spis na pouziti rekonstruovaneho modelu pro predikci okluze
- popat algoritmus predikce (vypocet uhlu vektoru k danemu 3D bodu a predpovezene pozici cile)
- dat obrazek, pohled shora, kde cil se pohybuje smerem za prekazku, zobrazit predikovanou pozici cile a vybarvit body, ktere nalezi prekazce

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Hardware and Software Architecture}

In order to support scalability and to meet the computational demands, the OLS is designed as a distributed system where each camera unit is equipped with its own desktop computer. Sections \ref{txt:hardware_topology} and \ref{txt:components_of_cu} describe the hardware architecture of the system and internals of the camera unit. The section \ref{txt:sw_pipeline} then presents the main software pipeline from the input of the operator to the estimated position of the target in 3-space.

%.........................................................................
%.........................................................................
\subsection{Hardware Topology} \label{txt:hardware_topology}

Considering the big picture of the system, OLS is designed as a simple two layer tree topology, where each node is represented as either tracking unit or overview unit (TU/OU) -- a standalone independent device continuously capturing the camera image stream, performing visual tracking and regulating the motion of the manipulator. All TU/OUs are interconnected via the Gigabit Ethernet switch. 

One of the TU/OUs is selected as the \textsc{master} -- a root of the tree topology which is continuously receiving the tracking estimates from all the TU/OUs and estimating the 3D location of the target (see Figure \ref{fig:hw_ols}). Additionally, \textsc{master} serves the purpose of the interface between the OLS and the human operators and enables them to manually initialize the system and control each TU/OU (using keyboard and/or joystick) during runtime if necessary. All the TU/OUs are synchronized using PTP, which is ensured by the hardware support provided by the Prosilica cameras.


%% The big picture diagram of the hardware components of the OLS
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.9\linewidth]{fig/hwsw_architecture.pdf}
	\caption{The hardware diagram of the OLS depicting the interconnection of the main building blocks -- the tracking and/or overview units (TU/OU).}
	\label{fig:hw_ols}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Components of the Camera Unit} \label{txt:components_of_cu}

As described in \ref{txt:devices} a camera unit itself consists of a P\&T unit, a camera, sensors used for stationing and a desktop computer running the OLS software. The P\&T unit and all the sensors communicate through the RS-232 bus with the controller (based on $\mu$P STM32F4007) serving the purpose of the hub aggregating the communication with the desktop PC. The controller was designed and produced by the Department of Measurement at Faculty of Electrical Engineering, CTU in Prague. The camera is connected via GigE bus and finally the desktop computer is linked to the Ethernet Switch via Ethernet bus (see Figure \ref{fig:hw_camera_unit})).

%% The diagram of the hardware components of the camera unit
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\linewidth]{fig/hw_camera_unit.pdf}
	\caption{The diagram of the hardware components of the camera unit depicting both the hardware devices and the communication standards.}
	\label{fig:hw_camera_unit}
\end{figure}

%.........................................................................
%.........................................................................
\subsection{Software Pipeline} \label{txt:sw_pipeline}

The operation of the OLS in the means of data flow is depicted in the Figure \ref{fig:pipeline}. The OLS is initialized by the human operator who directs the CUs to the desired target and starts the tracking process. All the selected tracking units then visually track the target, continuously stream current measurements to the \textsc{master} and regulate the motion of the P\&T unit so that the target would not disappear from their fields of view.

The localization component continuously estimates the location of the target in 3-space given the observations from the single tracking units. Given the knowledge of the target positions history and the suitable motion model the speed and future trajectory of the target can be estimated and used in order to refine tracking. The final estimate of the current target location is continuously emitted and presented to the operator. Should the operators decide to interfere, the peripherals (keyboard/mouse/joystick) can be used to manually adjust the tracking and regulation.

%% OLS runtime pipeline
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.95\linewidth]{fig/pipeline.pdf}
	\caption{The end-to-end pipeline of the OLS in the means of data sent among the separate software components.}
	\label{fig:pipeline}
\end{figure}


%=========================================================================
%=========================================================================
\chapter{Implementation} \label{txt:implementation}

/todo{sekce gui - vysvetlit, ze neni gui, ale nekolik oken, vypis aktualnich info, crosshait apod, ovladani mysi a klavesnici}

The whole system is built on the robotic framework Robot Operating System (for details see Section \ref{txt:robot_operating_system}). Since the ROS defines multiple conventions, restrictions and best practices the whole system design including the selection of a programing language, a programing and a communication paradigm, a target platform and a tool for physical simulations is impacted by the possibilities of this framework.

As of writing this text a current state of the implementation mainly builds on the virtual environment provided by the physical simulator Gazebo (see Section \ref{txt:application_of_gazebo}). When confronted with the overview of all subsystems making up the whole system presented in Section \ref{txt:system_overview}, so far the following parts are already designed, implemented and/or integrated:

\begin{itemize}
	\item manual control of the manipulator using peripheral devices
	\item manual selection of a target and distribution of its appearance to all CUs
	\item integration and utilization of a OpenTLD tracker
	\item the first prototype of triangulation of a 3D positions of a target within global frame
	\item integration of all subsystems
\end{itemize}

Furthermore a few additional tools were utilized and/or implemented as the necessary building blocks allowing for further development and testing:

\begin{itemize}
	\item a functional model of a whole system in Gazebo environment
	\item a standalone application for rectification
\end{itemize}


%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section{Robot Operating System} \label{txt:robot_operating_system}

Despite its name the ROS\footnote{The official website of ROS: \url{http://www.ros.org}} is not an operating system but rather a collection of open source libraries, tools and conventions which serve the purpose of a middlewear running alongside a real operating system, however it provides the programmer with the hardware abstraction,  low-level device control, implementation of commonly-used functionality,
message-passing between processes, and package management \cite{O'Kane201310}.

Since the original motivation for developing ROS was to support the collaboration among the experts in the field of robotics in the means of a common software platform~\cite{ROS-an-open-source-Robot-Operating-System}, a huge developer community has formed around ROS which resulted in wide-scale penetration of this framework as well as the support for a huge range of hardware devices.

%.........................................................................
%.........................................................................
\subsection{Application of ROS}

The OLS is designed to become a relatively complex system, thus it exhibits non-trivial implementation requirements such as a need to distribute the computation among multiple computers, the real time performance, integration with physical simulator, etc. Since ROS is a mature framework satisfying the most of theses requirements (see Table \ref{tab:ols_requirements_ros_features}), it was chosen as a main implementation platform.


{\renewcommand{\arraystretch}{1.5}
	\begin{table}[htbp]
		\centering
		\caption{The table lists the most important requirements of the OLS and describes how the ROS framework addresses them.}
		\begin{tabularx}{0.99\textwidth}{XX}
			\toprule
			\textbf{OLS requirements} & \textbf{ROS features} \\
			\midrule
			native support for hardware such as Prosilica camera, manipulator Flir PTU D46-70, joystick, keyboard & nodes implementing image capture from Prosilica cameras, capturing events from keyboards and joysticks \\
			modularity and reusability of source code & each subsystem is represented by a separate process (node), straightforward reusability \\
			distribution of computation among multiple computers & provides abstraction layer for distributing nodes across devices \\
			simple data exchange among subsystems & the publisher/subscriber paradigm \cite{O'Kane201310}, support for custom message formats \\
			real time performance & C++ implementation \\
			modelling and simulating the robot & custom language \texttt{URDF}\footnotemark ~for robot modeling, integration with Gazebo \\
			specifying a kinematic chain, heavy 3D transformation computation & native support for computing transformation between frames using package \texttt{tf} \\
			complex visualization, debugging & a visualization tool \texttt{rviz}\footnotemark ~for visualizing frames, transformations, robot models, image streams etc. \\
			physical simulation & integration with Gazebo \\
			\bottomrule
		\end{tabularx}
		\label{tab:ols_requirements_ros_features}
	\end{table}}
	
	\footnotetext{Modeling language \texttt{URDF}: \url{http://wiki.ros.org/urdf}}
	\footnotetext{Vsisualization tool \texttt{rviz}: \url{http://wiki.ros.org/rviz}}
	
	%.........................................................................
	%.........................................................................
	\subsection{Standard ROS Packages}
	
	ROS provides a wide range of standard packages for interaction with various hardware devices and performing various computations. The implementation of OLS utilizes following packages:
	
	\begin{description}
		\item[avt\_vimba\_camera]\footnote{Package avt\_vimba\_camera: \url{http://wiki.ros.org/avt_vimba_camera}} \hfill \\
		This package wraps the Vimba GigE SDK provided by Allied Vision Technologies, the manufacturer of the Prosilica series cameras, and allows the programmer to subscribe to the topic \texttt{camera\textbackslash image\_raw} and easily access the image stream.
		
		\item[keyboard]\footnote{Package keyboard: \url{http://wiki.ros.org/keyboard}} \hfill \\
		The package processes the keyboard events and exposes them via \texttt{keydown} and \texttt{keyup} topics.
		
		\item[joy]\footnote{Package joy: \url{http://wiki.ros.org/joy}} \hfill \\
		This package processes the events from a joystick and/or gamepad and exposes them via \texttt{joy} topic.
		
		\item[tf]\footnote{Package tf: \url{http://wiki.ros.org/tf}} \hfill \\
		The package manages the distribution of the states of all joints within all robot models (in case of OLS the kinematic chains representing the camera units) among all nodes as well as it performs the transformation between given frames on demand \cite{tf}.
		
	\end{description}
	
	
	%-------------------------------------------------------------------------
	%-------------------------------------------------------------------------
	\section{Nodes Interaction Design}
	
	
	The system is divided into multiple ROS nodes (processes running in the operating system) with the aim to loosely resemble the hardware components. Five namespaces are used, one \texttt{master} namespace and four \texttt{camera\_unit\_N} namespaces, where $N \in \{0, 1, 2, 3\}$ identifies a unique camera unit. The overview of the system architecture from the perspective of the ROS namespaces, nodes, messages and services is depicted on Figure \ref{fig:sw_ols}.
	
	%% The diagram of the software components - ROS nodes - and ROS topics.
	\begin{figure}[htb]
		\centering
		\includegraphics[width=14.5cm]{fig/sw_ols.pdf}
		\caption{The diagram of the software components represented by the ROS nodes. The communication among topics is implemented using ROS topics (normal arrows) and ROS services (dashed arrows).}
		\label{fig:sw_ols}
	\end{figure}
	
	The nodes running within the \texttt{master} namespace serve the purpose of the access point for an user (an operator) as well as the main controller of the whole system. The node \texttt{controls} keeps the information about the tracked targets and distributes the workload to the separate camera units based on their position and orientation with regards to the newly discovered target. The node \texttt{position\_estimation} calculates the triangulation and estimates the 3D position of the target. The node \texttt{GUI} implements the graphical user interface allowing the operator to control the system and the nodes \texttt{joy} and \texttt{keyboard} process the events from peripherals.
	
	Each of the namespaces \texttt{camera\_unit\_N} controls a separate camera unit. The namespace contains the standard node \texttt{avt\_vimba\_camera} processing the input image stream from Prosilica camera and it implements the node \texttt{manipulator}, which controls the manipulator and publishes its state, and the node \texttt{detection\_and\_tracking}, which performs the computationally most expensive tasks of the object detection, learning its appearance and tracking.
	
	
	%-------------------------------------------------------------------------
	%-------------------------------------------------------------------------
	\section{Application of Gazebo} \label{txt:application_of_gazebo}
	
	Gazebo\footnote{The official website of Gazebo: \url{http://gazebosim.org}} is a physical simulator developed by the OSRF\footnote{Open Source Robotic Foundation: \url{http://www.osrfoundation.org/}} providing the tools to model and simulate robots in both indoor and outdoor environment. The simulator has been developed since 2002 and today represents a mature system with wide penetration and support, while being distributed as open source and freeware. 
	
	Since the Gazebo is distributed also as one of the standard packages of the ROS framework it is straightforward to integrate the simulation environment with the already implemented ROS nodes. There are multiple advantages of using the simulator over a development using a real hardware, the main motivations were as follows:
	
	\paragraph{testing a tracker} The Gazebo provides the ROS plugin simulating an RGB camera, which captures the virtual scene and publishes a stream of rendered images. Thus it can be used to test an object tracking algorithm using arbitrarily complex environment and moving objects (see Figure \ref{fig:gazebo_camera_stream}).
	
	%% The screenshot of Gazebo scene and image streams from cameras.
	\begin{figure}[htb]
		\centering
		\includegraphics[width=14.5cm]{fig/gazebo_camera_stream.png}
		\caption{The screenshot of a Gazebo simulation (left) consisting of a virtual environment (the gas station), a flying object (the red sphere) and four manipulators. All four virtual camera streams are displayed real-time using \texttt{rviz} tool (right).}
		\label{fig:gazebo_camera_stream}
	\end{figure}
	
	\paragraph{testing a manipulator} It is a good practice to include a real hardware in the simulation during the development \cite{on_hw_in_the_loop}. Both actuators and sensors would be difficult to simulate properly, moreover a real manipulator is constrained in terms of the operational range (see Section \ref{txt:camera_unit}), maximum acceleration and speed and communication throughput so it is necessary to thoroughly test its performance. This so called hardware-in-the-loop simulation reveals whether the implementation of motion control is correct and whether the possibilities of the manipulator suffice to track arbitrarily fast (simulated) objects.
	
	\paragraph{testing a triangulation} Thanks to the Gazebo it is possible to simulate a flying object with a-priory set trajectory and evaluate the precision of a position estimation algorithm using comparison between the estimated target position and a ground-truth.
	
	%-------------------------------------------------------------------------
	%-------------------------------------------------------------------------
	\section{External Libraries} \label{txt:external_libraries}
	
	Besides the framework ROS a few other publicly available libraries are used within the implementation.
	
	\paragraph{OpenCV} Open Source Computer Vision Library\footnote{The official website of OpenCV: \url{http://opencv.org}} is a free open source library providing algorithms for image processing, computer vision and machine learning. Version 2.4.11 is used as it is a component of ROS Indigo.
	
	\paragraph{Eigen} This open source C++ template library\footnote{The official website of Eigen: \url{http://eigen.tuxfamily.org}} implements the data structures and methods for fast and convenient solving of linear algebra problems.
	
	\paragraph{OpenTLD} The OpenTLD library\footnote{The official website of OpenTLD: \url{http://www.gnebehay.com/tld}} represents an open source C++ implementation the TLD tracking algorithm (see Section \ref{txt:detection_and_tracking}).	
	
	\paragraph{Serial} A cross-platform library Serial\footnote{The official website of Serial: \url{https://github.com/wjwwood/serial}} implemented in C++ provides the API for interfacing with RS-232 serial like ports. It is used to control the manipulator.
	
	%=========================================================================
	%=========================================================================
	\chapter{Experiemtnal Results}
	
	\vata[28]
	
	%=========================================================================
	%=========================================================================
	\chapter{Conclusion} \label{txt:conclusion}
	% 1 - 2 ns
	
	\todo{zhodnocení z pohledu dalšího vývoje projektu}
	\todo{student uvede rovněž návaznosti na právě dokončené projekty (řešené v rámci ostatních diplomových prací v daném roce nebo na projekty řešené na externích pracovištích).}
	\todo{zhodnocení dosažených výsledků se zvlášť vyznačeným vlastním přínosem diplomanta}
	
	
	This work presents a novel system for autonomous optical localization of aerial targets. The principle of triangulation is well known and widely used, however the main contribution of the system is the fact it is designed to operate autonomously from detecting the object to estimating its position. As the first step, a suitable placement of available camera units had to be designed and the proper hardware components, which each camera station consists of, had to be selected. A model of the camera unit represented by a kinematic chain was proposed and all camera units were then rectified in order to minimize the difference between the model and the real construction of the camera units.
	
	As fo the software solution, the most promising computer vision techniques for both detection and tracing of the target were proposed and one of them, a free implementation of the TLD algorithm was eventually integrated in the system. The architecture of the system based on the ROS framework was designed and implemented and the physical simulation software Gazebo was used to experiment with the system placed in a virtual scene.
	
	As far as the further development is concerned, the autonomous detection algorithm will be implemented and the tracking algorithm will be improved on since the OLS system expects a static background (as opposed to the TLD) so the background modeling could be incorporated to further improve the tracker performance. The subsystem for target handoff among the cameras will be designed and implemented and the localization algorithm will be improved on to reflect the uncertainty of the locations and orientations of the separate joints in the kinematic models and to provide the result in global geographical coordinates (most likely UTM). A proper dataset will be acquired and the system will be tested thoroughly.
	
	\vata[5]